{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our steps\n",
    "- Initialize weights randomly\n",
    "- Give the model a char pair (input char & target char. The target char is the char the network should guess, its the next char in our sequence)\n",
    "- Forward pass (We calculate the probability for every possible next char according to the state of the model, using the paramters)\n",
    "- Measure error (the distance between the previous probability and the target char)\n",
    "- We calculate gradients for each of our parameters to see their impact they have on the loss (backpropagation through time)\n",
    "- update all parameters in the direction via gradients that help to minimise the loss\n",
    "- Repeat! Until our loss is small AF\n",
    "\n",
    "## The code contains 4 parts\n",
    "- Load the trainning data\n",
    "    - encode char into vectors\n",
    "- Define the Recurrent Network\n",
    "- Define a loss function\n",
    "    - Forward pass\n",
    "    - Loss\n",
    "    - Backward pass\n",
    "- Define a function to create sentences from the model\n",
    "- Train the network\n",
    "    - Feed the network\n",
    "    - Calculate gradient and update the model parameters\n",
    "    - Output a text to see the progress of the training\n",
    "    \n",
    "    \n",
    "## Load the training data\n",
    "The network need a big txt file as an input.\n",
    "\n",
    "The content of the file will be used to train the network.\n",
    "\n",
    "I use Methamorphosis from Kafka (Public Domain). \n",
    "\n",
    "Because Kafka was one weird dude. I like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 137629, unique: 81\n"
     ]
    }
   ],
   "source": [
    "data = open('kafka.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "print (\"Length: {}, unique: {}\".format(data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode/Decode char/vector\n",
    "Neural networks operate on vectors (a vector is an array of float)\n",
    "\n",
    "So we need a way to encode and decode a char as a vector.\n",
    "We'll count the number of unique chars (vocab_size). \n",
    "\n",
    "That will be the size of the vector. The vector contains only zero exept for the position of the char wherae the value is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'E': 0, 'V': 1, 'a': 2, 't': 3, 'C': 4, '\\n': 5, 'i': 6, 'B': 7, 'P': 8, 'U': 9, 'Y': 10, 'M': 11, 'R': 12, '6': 13, 'x': 14, '0': 15, \"'\": 16, '?': 17, '5': 18, '2': 19, '1': 20, 'o': 21, 'Ã': 22, 'N': 23, 'X': 24, '%': 25, '@': 26, 'z': 27, '(': 28, ' ': 29, 'u': 30, 'w': 31, 'e': 32, 'S': 33, 'J': 34, 'v': 35, 'm': 36, 'G': 37, 'y': 38, '7': 39, 'H': 40, 'W': 41, '4': 42, 'f': 43, 'A': 44, 'F': 45, 'p': 46, 'b': 47, 'T': 48, ',': 49, '9': 50, 'j': 51, '§': 52, '*': 53, 'Q': 54, '.': 55, 'O': 56, '-': 57, 's': 58, '!': 59, 'n': 60, 'I': 61, 'h': 62, 'D': 63, '/': 64, ')': 65, ':': 66, 'k': 67, 'g': 68, 'c': 69, '$': 70, 'l': 71, 'q': 72, '3': 73, 'r': 74, 'K': 75, 'd': 76, '8': 77, '\"': 78, ';': 79, 'L': 80}\n",
      "{0: 'E', 1: 'V', 2: 'a', 3: 't', 4: 'C', 5: '\\n', 6: 'i', 7: 'B', 8: 'P', 9: 'U', 10: 'Y', 11: 'M', 12: 'R', 13: '6', 14: 'x', 15: '0', 16: \"'\", 17: '?', 18: '5', 19: '2', 20: '1', 21: 'o', 22: 'Ã', 23: 'N', 24: 'X', 25: '%', 26: '@', 27: 'z', 28: '(', 29: ' ', 30: 'u', 31: 'w', 32: 'e', 33: 'S', 34: 'J', 35: 'v', 36: 'm', 37: 'G', 38: 'y', 39: '7', 40: 'H', 41: 'W', 42: '4', 43: 'f', 44: 'A', 45: 'F', 46: 'p', 47: 'b', 48: 'T', 49: ',', 50: '9', 51: 'j', 52: '§', 53: '*', 54: 'Q', 55: '.', 56: 'O', 57: '-', 58: 's', 59: '!', 60: 'n', 61: 'I', 62: 'h', 63: 'D', 64: '/', 65: ')', 66: ':', 67: 'k', 68: 'g', 69: 'c', 70: '$', 71: 'l', 72: 'q', 73: '3', 74: 'r', 75: 'K', 76: 'd', 77: '8', 78: '\"', 79: ';', 80: 'L'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:ix for ix, ch in enumerate(chars)}\n",
    "ix_to_char = { ix:ch for ix, ch in enumerate(chars)}\n",
    "\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finaly we create a vector from a char like this:\n",
    "The dictionary defined above allosw us to create a vector of size 61 instead of 256.\n",
    "\n",
    "Here and exemple of the char 'a'\n",
    "\n",
    "The vector contains only zeros, except at position char_to_ix['a'] where we put a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['A']] = 1\n",
    "\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the network\n",
    "The neural network is made of 3 layers:\n",
    "- an input layer\n",
    "- an hidden layer\n",
    "- an output layer\n",
    "All layers are fully connected to the next one: each node of a layer are conected to all nodes of the next layer.\n",
    "    \n",
    "The hidden layer is connected to the output and to itself: the values from an iteration are used for the next one.\n",
    "    \n",
    "To centralise values that matter for the training (hyper parameters) we also define the sequence lenght and the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters are adjusted during the trainning.\n",
    "- Wxh are parameters to connect a vector that contain one input to the hidden layer.\n",
    "- Whh are parameters to connect the hidden layer to itself. This is the Key of the Rnn: Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.\n",
    "- Why are parameters to connect the hidden layer to the output\n",
    "- bh contains the hidden bias\n",
    "- by contains the output bias\n",
    "You'll see in the next section how theses parameters are used to create a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.1\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.1\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.1\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "The loss is a key concept in all neural networks training. It is a value that describe how good is our model.\n",
    "The smaller the loss, the better our model is.\n",
    "(A good model is a model where the predicted output is close to the training output)\n",
    "During the training phase we want to minimize the loss.\n",
    "\n",
    "##### The loss function calculates the loss but also the gradients (see backward pass):\n",
    "- It perform a forward pass: calculate the next char given a char from the training set.\n",
    "- It calculate the loss by comparing the predicted char to the target char. (The target char is the input following char in the tranning set)\n",
    "- It calculate the backward pass to calculate the gradients\n",
    "\n",
    "##### This function take as input:\n",
    "\n",
    "- a list of input char\n",
    "- a list of target char\n",
    "- and the previous hidden state\n",
    "\n",
    "##### This function outputs:\n",
    "\n",
    "- the loss\n",
    "- the gradient for each parameters between layers\n",
    "- the last hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev): \n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    \n",
    "    hs[-1] = np.copy(hprev)\n",
    "    \n",
    "    #initialize loss to 0\n",
    "    loss = 0\n",
    "    \n",
    "    #forward pass\n",
    "                                                                                      \n",
    "    for t in range(len(inputs)):\n",
    "        # encode in 1-of-k representation (we place a 0 vector as the t-th input)   \n",
    "        xs[t] = np.zeros((vocab_size,1))    \n",
    "        # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "        xs[t][inputs[t]] = 1 \n",
    "        # hidden state \n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)   \n",
    "        # unnormalized log probabilities for next chars\n",
    "        ys[t] = np.dot(Why, hs[t]) + by   \n",
    "        # probabilities for next chars \n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) \n",
    "        # softmax (cross-entropy loss) \n",
    "        loss += -np.log(ps[t][targets[t],0])  \n",
    "    # backward pass: compute gradients going backwards    \n",
    "      #initalize vectors for gradient values for each set of weights \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        #output probabilities\n",
    "        dy = np.copy(ps[t])\n",
    "        #derive our first gradient\n",
    "        dy[targets[t]] -= 1 # backprop into y  \n",
    "        #compute output gradient -  output times hidden states transpose\n",
    "        #When we apply the transpose weight matrix,  \n",
    "        #we can think intuitively of this as moving the error backward\n",
    "        #through the network, giving us some sort of measure of the error \n",
    "        #at the output of the lth layer. \n",
    "        #output gradient\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        #derivative of output bias\n",
    "        dby += dy\n",
    "        #backpropagate!\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "        dbh += dhraw #derivative of hidden bias\n",
    "        dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Whh.T, dhraw) \n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients  \n",
    "        \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sentence from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " ,03Ohk/\n",
      "Pq9UyLEo.wR3*V\n",
      "YH,eX(U§zK)VsdQiFki uT!QF8x2OQAWLFXW0:)e2'@Qt.P$\" ::fi?766wu8\"r@L§YXA2GB6LK§whR:8q8icy)t:5Bfxls\"z4OXMLdP :\"lMQyX0PE3/k'i3WJkf(4F1rQgWK6§5Ihm91(NVweG:UT/)044rJBz;!,%%zuufgk3'y)t) \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"                                                                                                                                                                                         \n",
    "    sample a sequence of integers from the model                                                                                                                                                \n",
    "    h is memory state, seed_ix is seed letter for first time step   \n",
    "    n is how many characters to predict\n",
    "    \"\"\"\n",
    "    #create vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for our seed char\n",
    "    x[seed_ix] = 1\n",
    "    #list to store generated chars\n",
    "    ixes = []\n",
    "    #for as many characters as we want to generate\n",
    "    for t in range(n):\n",
    "        #a hidden state at a given time step is a function \n",
    "        #of the input at the same time step modified by a weight matrix \n",
    "        #added to the hidden state of the previous time step \n",
    "        #multiplied by its own hidden state to hidden state matrix.\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        #compute output (unnormalised)\n",
    "        y = np.dot(Why, h) + by\n",
    "        ## probabilities for next chars\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        #pick one with the highest probability \n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        #create a vector\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        #customize it for the predicted char\n",
    "        x[ix] = 1\n",
    "        #add it to the list\n",
    "        ixes.append(ix)\n",
    "\n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    print ('----\\n {} \\n----'.format(txt, ))\n",
    "\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [56, 60, 32, 29, 36, 21, 74, 60, 6, 60, 68, 49, 29, 31, 62, 32, 60, 29, 37, 74, 32, 68, 21, 74, 29]\n",
      "targets [60, 32, 29, 36, 21, 74, 60, 6, 60, 68, 49, 29, 31, 62, 32, 60, 29, 37, 74, 32, 68, 21, 74, 29, 33]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print (\"inputs\", inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print (\"targets\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 109.86146025086015\n",
      "----\n",
      " n CI§2V7Y5j0w6BbI8fV4Ymft0BKxHw;3?S!XHkdRER\"yfz6l-bwaQj-wj5H\n",
      "§yA3;s8f7RYlp2laFYcV06\n",
      "g9@QeI8yLuCv.kN8?6\n",
      "@Ãe*\"hBp./nc\"tItl*Q@,I7RD!-gV\"u3o8@cK$o\n",
      "UB;$nOp2y;Spa\n",
      "YY820$,o9:7fT6 \"VDe(hqU%c37R7poM,eyH;)Qq1,a \n",
      "----\n",
      "iter 1000, loss: 87.40981938408657\n",
      "----\n",
      "  thonsd hhiy nssmr he pet sim\n",
      " aate \"R amltie t hat oy n no thos nt hisp habid tpfgn redoly ps,reugrudistronsg tS rhe teua. frolt tuen, fme -, au hcege fen txtecuyo lovam aacte%iplun e inie sohnaauHem \n",
      "----\n",
      "iter 2000, loss: 70.23265935417703\n",
      "----\n",
      " ow, aimrele wen che, e etiw ras theulg. Ntherlme y ilfmed pet the twe wathe the inmew mat wee fSed mirouthe o feroad ogo tus alen hant balerecos wrrotheturged ay anwame doud aul srily fhit heas that e \n",
      "----\n",
      "iter 3000, loss: 61.11582728807139\n",
      "----\n",
      " ll Gr walls. \"torais the pot fo him lpache sullt kaspy sot, wat couln; wend ralf worit s the sory shirved wem to al yor. juad sidl thetW anor feengher larsirtund, od ath tohr ore.r.\n",
      "TX- wad whe.\n",
      "?Tinf \n",
      "----\n",
      "iter 4000, loss: 56.306625824517816\n",
      "----\n",
      "  in oring fit seerstosls hampen af ine dad to keimup pit the wiso siem. wad tnol mDoot riit of masckit themath ut Gr rore boon erese os wous wishey git tothar ingorefor at an hum ar the th he rave .i2 \n",
      "----\n",
      "iter 5000, loss: 58.17658754415058\n",
      "----\n",
      " ok intho fore.m ore ininsl atendo bidadpsios midptchack ane boa'd out s aines coseisitis byose thee watht A*oves lf ther\n",
      "aGd Gr*ajf\n",
      "york \"batisd ave. Sbeyilisiscyf\n",
      "evonw firepnnat on cheol a ofrery ch \n",
      "----\n",
      "iter 6000, loss: 58.49767305783001\n",
      "----\n",
      " iod An thas th uand as whe wof pavit suedeat s. Abek \"xas wIf wo movr, cord anthed ing wang and. Thwinhing bots on pfeking. \"omd he her mhoblet lisherko aon his lowat in wand. Aat and bed over's: best \n",
      "----\n",
      "iter 7000, loss: 54.10051983350956\n",
      "----\n",
      " ere, firay teago hiv wallo himp soulld heri ay asid wary ham, the Got hecken,y and it to enoting Greled houch the llon, the reefrefivis dom he wat ppugring Gever hith atiemstod it. -t the forw, Gregha \n",
      "----\n",
      "iter 8000, loss: 50.9016067971123\n",
      "----\n",
      " her thing lol toing l, suit nhe faswh tom atter walke wast operlintimilg dive hes vit in ais she wemeedery freater to gas oppto toonr ssecead was to had them inwor had mere beder aby was he. He freall \n",
      "----\n",
      "iter 9000, loss: 49.38842194978847\n",
      "----\n",
      " !, ald sfound, in wangerttar op. Bhe se coul'kly, woor; on was halp opupingediis thabiet whather bosedised the the mowed ihd suite ofrouldprentill inlko fichamterfubey chenecnriind evenst tonincero un \n",
      "----\n",
      "iter 10000, loss: 48.83328639042105\n",
      "----\n",
      " y to fotle lougel gothirg. So wath the afore on feated fast at, Gregors bre at nother hing mess anbne whim heund \"massay moor thangirs oning rothew room nally hass enthar's nithar Greger aps sto she f \n",
      "----\n",
      "iter 11000, loss: 55.07252892951677\n",
      "----\n",
      " rly an\n",
      "E\n",
      "pparvil or to. Fot7 EN Euve\n",
      "ftaspay mon %ralle foret ofsent ontisabithott\n",
      "eng toEt Grchive wint,reawn\n",
      "G/he tanit.\n",
      " 1L; and womed\n",
      "ston Groinben ubexcold woult the clomaind Foond ack\n",
      "5ey the Fo \n",
      "----\n",
      "iter 12000, loss: 52.19479349315641\n",
      "----\n",
      " sherero wyorkef a mters. dalf the ghe lemear sRaw aed to hes have to as wesin; the cans, sheco qound think whit thene the dard or hat, If thing to dad the the ppe ger'shove west and iom) had the anted \n",
      "----\n",
      "iter 13000, loss: 49.1220942473955\n",
      "----\n",
      " w If dustly tho as nobevereafathat a fay when last he lfect thith not my merghe, eld whath his forming, to heed at ovely - be apred in chat\" as mereably homot thay un sees she brent- in fliw not ae fo \n",
      "----\n",
      "iter 14000, loss: 47.300289735131216\n",
      "----\n",
      " n his wutr; turther at ut was and even Than to then isit me thaw this mastay dears.\n",
      "\n",
      "Ansiibelf eren shere has meroo to sanar wispe har sto aven entiet noos room hery would the dore his lico fere an fo \n",
      "----\n",
      "iter 15000, loss: 46.705467972379665\n",
      "----\n",
      " ied with and hom dagoreds ka, the the mclyors Gregor at ore chait cloor it core tind in had syey legard her\", winy thand ham his pay and toce her his at him. She wanom, toodt elforertiod thant she wen \n",
      "----\n",
      "iter 16000, loss: 49.555548874254015\n",
      "----\n",
      " it Grmith; \"Thee yighing heryen comy to hely gerk wany the decperyevent be pabten, ay, Sloogerty rojoc peaperss she tomenting chece wa conefild lelf of cenoss fors/oble to her's to wighat the is prome \n",
      "----\n",
      "iter 17000, loss: 51.30234171699039\n",
      "----\n",
      " ady, fracond sled an igstor hag teots the quchave to wustiliagoll tine \"lutive ban, his inserd with brewire buch stle sGuttite bady wilt ko  aint be noge of whingt hips to. He wall had he marly and ai \n",
      "----\n",
      "iter 18000, loss: 48.605859604043275\n",
      "----\n",
      " . He reant, thet morking his hald, But juks her nowi)g; and tamore doom the siit had ated, the out Gregor histsarlisist to faldo Aly it to had tioo kas teat there wo of itle all to tak. Gr he nofclrin \n",
      "----\n",
      "iter 19000, loss: 46.48987794654541\n",
      "----\n",
      "  and tot gore doat of the he lapkeat mork, as tint baig then hid was ca ourying cromen in lealp. Slowand, ana rowering most unfess tho gry nee dary uplo inthing no halt way one be ham as his sered ext \n",
      "----\n",
      "iter 20000, loss: 45.697974101782634\n",
      "----\n",
      " ding Greger be brow fronkily ovente the's8 of tregad into his morcine was soont hid mxed and the on Looke it her that mor's stmround out mamese wiet agly the desed of his flout, times foresed the ghe  \n",
      "----\n",
      "iter 21000, loss: 45.57948618241222\n",
      "----\n",
      " s intaitinck eledse seclast beist thatside the whoutt onc auping, and sow he doong , droond couts to door a whiemy. or ap!o,. He weon it stain herded with not the begoed it wist ow ally it of in this  \n",
      "----\n",
      "iter 22000, loss: 50.724271153070646\n",
      "----\n",
      " lente gruplir\n",
      " eppord ard thes\n",
      "'f thes brove flews Hoy proumay\n",
      " a Ung to then. Mre sid lant plmospersibate or hik ingere) thef inefalited arding and.u@ IN WiGg in or arPiouub\n",
      "\n",
      "obyom. AY Finkly drintar \n",
      "----\n",
      "iter 23000, loss: 48.77025189638756\n",
      "----\n",
      " er get Boct canen read the sided on to ferss and of heir of his leverked to ulxo \"so canligengor te had lee strougher, courtt all of the perait me's peallet of kuchich puhim; tame to to to his fation' \n",
      "----\n",
      "iter 24000, loss: 46.271653148898174\n",
      "----\n",
      "  he comen him. fod trago the sork when that norme and bmese ally beent calid; they had inat the wisher dant, his fleabelly, that evere lotibut risher lucht dlemsllees a berecemned, soulce he apeus mem \n",
      "----\n",
      "iter 25000, loss: 44.75123916858349\n",
      "----\n",
      " ion have all that aly he to eve sore illong herre-w Gusso what, whtAr. Hp allen to chaise frabef mood her chat to in the woble or lor she be ary and be his fall the mall cants of was Ifouf the cluke y \n",
      "----\n",
      "iter 26000, loss: 44.39674963643255\n",
      "----\n",
      " d fort to s awlithedid. Hew that her his wer to besp posher moo'd seithen to lest Gregor'mong is to awher, go lavelf thenthenes at tho the wan it toom in or foo toon, she withent comerent rochang thut \n",
      "----\n",
      "iter 27000, loss: 46.8204721712387\n",
      "----\n",
      " oud tish; it in thelO and on the ceponing inent \"Nomsowhin's regom of the othing Gregont.\n",
      "\n",
      "IRh wir. She Gregor, Loudlinad had agane or's effighind. Sy asf sooked Pnacksy amming the bricsull Promers se \n",
      "----\n",
      "iter 28000, loss: 48.73678456080893\n",
      "----\n",
      "  The digexmable and but or frowf,r  somet. I's it stord hich shaplitred the call hooy of. Hlaowach it when whing whain thater reqomst to stithting then the lile, he Projoatlang. He room thenced the mo \n",
      "----\n",
      "iter 29000, loss: 46.58387426524821\n",
      "----\n",
      " sed the mistd; tad eJbels. He sacw? I'm ang mothas this wistle seest becibuthe nowly even mister, the teareally realet in wost maops sidech abut, arlouth him. - caarle didedalt cald whit fores stiind  \n",
      "----\n",
      "iter 30000, loss: 44.76702404997205\n",
      "----\n",
      " the dighel fron sifdloten, he congor at hos soob nago could him himn clations, has peacany heed uneve \"terse hive lither than cleep, hissibed to lernyting his mith Guthon had a loon tiswaad where, fro \n",
      "----\n",
      "iter 31000, loss: 44.12827444050142\n",
      "----\n",
      " antent usce stilple moon, mquisester was dest hercen nose as fer of rate offeely abceay theme had in would roor with she anged acarpings in prating he looks was would the lots the; arout she could ald \n",
      "----\n",
      "iter 32000, loss: 44.06074749274076\n",
      "----\n",
      " ng nollwing bath atce made was not ansatt hand reatitt in an the to ah sid to but ho cook the hay had state lutily noldse'pt ary som even her's it ins cout, shly live had he keox. Gren's hat it was be \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 33000, loss: 48.74249857293732\n",
      "----\n",
      " bed com the wat wneare outer werny\n",
      "daillicp he se the ousll.\n",
      "\n",
      "Heazre\n",
      "niged of the plod nectinellyy\n",
      "- midcomatt out the liking sid andu.  Mabl.  !900Vrocerth the Projse toddimistrot ention, wive contlu \n",
      "----\n",
      "iter 34000, loss: 47.19072763833095\n",
      "----\n",
      " d wisterce he door.\n",
      "\n",
      "Her the was meatly roon growing, hawf on bepent his fathe) it qustom. Mrt qunvert kive ppened in baacelfy though the of tho det sfoat hid he a cakes of the, fall and his payisict  \n",
      "----\n",
      "iter 35000, loss: 44.79448606399877\n",
      "----\n",
      " s. nis doof s iful toing wource. uh quit bundely deally promay tomen, theice hard steemechand atibed thatu onemert was evre even of keatil arore ive to thending. Ibe fle shat Them, and buet and his ,  \n",
      "----\n",
      "iter 36000, loss: 43.39700818616252\n",
      "----\n",
      " eled, painll qulit seaps to s ither un hamvey.\" ghe deathen it; tay te. He couther nesst coment noweis to his saving and Gregor to mosher that heud with these pale tume that anrsseenfar would sfother  \n",
      "----\n",
      "iter 37000, loss: 43.19920540989028\n",
      "----\n",
      " \n",
      "Ther eveny the to ghey his nece. ,, he moll got of cakablly in his rathib out ploncorren med her actibut at his aused ffrermonging it - some theme the adresed, ofle he pane hes litiwither had limpen  \n",
      "----\n",
      "iter 38000, loss: 45.272333146478054\n",
      "----\n",
      " n the all far with on towand, of he wesk yeppey, of anding in corject on baksed beterst becom duom oum the sporeny, but, worked copstnis for to plom, turn conted his lus was smo keleed. WOs with same  \n",
      "----\n",
      "iter 39000, loss: 47.3116689505596\n",
      "----\n",
      " extregs anatiee Gutenser that shight, incatheb'ld to himsef tabed and quomar was the whan hel his be, bresing houp desing ho do lematelps in to te preirsing\n",
      "d stecoldith, backwore thath but as  beckme \n",
      "----\n",
      "iter 40000, loss: 45.21176626836265\n",
      "----\n",
      " enclied on sum only she wantad, exsidg just evening, the potent Gregors; a fect ten stref learefwf, dlocko, aild beever hele owent. He dow tont his sained abould sut Gregor mormodg ang and sho mich bu \n",
      "----\n",
      "iter 41000, loss: 43.40766134502448\n",
      "----\n",
      " ly whebeved ther, his nor undert intine buterle baviel. Weme \"Greloor on evend had would fromer fid as of of caren. 3not as reselling to commed to chintit contun unlsed Gregor by shis fored she alid h \n",
      "----\n",
      "iter 42000, loss: 42.97298053527473\n",
      "----\n",
      " t by theres, wojemn brougho, ress apling had a her as. Do hack mive rutily tearning it sust to motreing Gregor step: Thetletpsy the riextove, aniss waed wand a frivise atir botely for ceaply was it in \n",
      "----\n",
      "iter 43000, loss: 42.90813985194247\n",
      "----\n",
      " y would up somenound. Widl of reakies lidtoked bented he was show at loting gepsund I've wo him that he dans unit Gregorg\n",
      "Gregoet thkigis mork whing anow shere intion to may ceay incume chateer, and a \n",
      "----\n",
      "iter 44000, loss: 47.19605012403668\n",
      "----\n",
      " endes and gromancaintions@ctil armeagenar\n",
      "at ponin wesppor his quicalf and thaceclightsed tece to the beamied atantion, you moug the camy the cevurottiratunect tevace mants of. TTw. \"THe FaI't rodicse \n",
      "----\n",
      "iter 45000, loss: 45.96581946850399\n",
      "----\n",
      " m \"iochan sore. He chiel, root lonk ight erenoust he gon isws on sidit mesk farns lan to he this tento be nos\n",
      "lippeen on carm the atsord neslarwa of un dessother thas the cuture probents leght of in h \n",
      "----\n",
      "iter 46000, loss: 43.72638230254366\n",
      "----\n",
      " ho culawn aw\n",
      "was sifres wome \"this daarbece, unrnat mast and it of to it onle Gregor for to cant\"; kereded gover for tore byeen? And plation's with ally then 1.F,. Thet beents for her more comion the  \n",
      "----\n",
      "iter 47000, loss: 42.40135214934065\n",
      "----\n",
      " ot witl Inro hact, with be if heet? gow of they his fare have light hely of trowesent, in the wiver mik. Gregor.\n",
      "\n",
      "But ago fitle a toult to wa had wimed, at to his sacs could doost her ow the cours one \n",
      "----\n",
      "iter 48000, loss: 42.243929498972086\n",
      "----\n",
      " ted thenciment whie had buar and wadyuselins, she centhing, the rewere And and froomel the loom beabded to fest rank searlow wIbeash, in. As werg at. Hed the could mothan to chaible Grying roomed-tion \n",
      "----\n",
      "iter 49000, loss: 44.14199996417712\n",
      "----\n",
      "  Ate the darks sulcess the bocand. Probespeadiled wally capel \"I once ompeist I withighe best Liefoving the camicer the woulle to soled lork lonke to dipl this tor the comppee ly rrigepledd chile the  \n",
      "----\n",
      "iter 50000, loss: 46.25372971171782\n",
      "----\n",
      " other, removen shoves alonether form.\n",
      "\n",
      "THed for frock claight whel the ferseply, hom, he motemarbermazer rougher in he wosle sought at to the way soulduttod it's dosed youd or nop it whing hery to\"s,  \n",
      "----\n",
      "iter 51000, loss: 44.316693519954384\n",
      "----\n",
      " y waccerat ghen acted the marksed.\n",
      "\n",
      "*Yot his have oully and wese statets his epet of had now s\"ong oup or and and his see, a rupemardiaged not, he was sostontaln it to the compelayed this the. \n",
      "lo str \n",
      "----\n",
      "iter 52000, loss: 42.57352272595899\n",
      "----\n",
      " ad for necking apard door. Op a out tfittlut fore and and were ele.\n",
      "Prigeaten and rocanaaf the all he lorge to proildy to he or wher of they get had inco sire for painge they to lawn beatedion was coo \n",
      "----\n",
      "iter 53000, loss: 42.215651413069914\n",
      "----\n",
      " ie form. Af whed of jims growing of any just anintt morried.\n",
      "\n",
      "Gregor fur, swen luket and even ullenot les into someve mad it, alle of out hate, appots go morsessed uryoumed avery frerstrom for far sis \n",
      "----\n",
      "iter 54000, loss: 42.12494271071167\n",
      "----\n",
      " hey opmis to cand lighther his sigly mef a felt the they wan his face tos lepeed have as we clitefsely toads loby elesere slidked that his nike, and a comst stroughan a sanks viedsps bucher preepandy  \n",
      "----\n",
      "iter 55000, loss: 46.07534312329816\n",
      "----\n",
      " ersy listen o deting to mafpancxinmat ontor with the tibed andent atung. yould theenpundlef Grepeeding, and mawendoning ellig.  Nom\n",
      "don yoy ondrytayet, that Projeleed malk to nocaallubest the Greg-te  \n",
      "----\n",
      "iter 56000, loss: 45.09859231396338\n",
      "----\n",
      " en you ipation have up undert stratf\", renknded one Gutey could here sentert arous, in to the Fouth it, reelly sark all\n",
      "sway in eatsed dent, slould!\"\n",
      "\n",
      "Gregor not Sanging an bedrestlabler hawwore bely. \n",
      "----\n",
      "iter 57000, loss: 42.95626880551411\n",
      "----\n",
      " ur of the wescing mist thre, ristrong effrrofed would with agco coull in a he had siagle; car to paing-ring brom ald s.  wasiching notht contlesed took, his gom, them had a peltraich shen shey its was \n",
      "----\n",
      "iter 58000, loss: 41.730393600258516\n",
      "----\n",
      " nle, but become to clenvearing and frundle or: nean to the sow. Grraked har's feap at Gregor's ay than he ople to s iff he was she be s frece anl forserperis, \"licalled in with payed they ote he couls \n",
      "----\n",
      "iter 59000, loss: 41.57059057709985\n",
      "----\n",
      " ud ut, stet there derenpeded he into swanong riestodent him had to list ocked tooud3 atle he polfot the riect sam-id what thepen was he room it she st optor's ritate intertorn aly it was'me would stus \n",
      "----\n",
      "iter 60000, loss: 43.35227890541815\n",
      "----\n",
      " osting ald flet as wistered they works or they bested anxo meft the groakinat and selying \"Gregor's fothim, and where. It surs was lackpataythe ten the carming the ro-te \"- chive its the most but in l \n",
      "----\n",
      "iter 61000, loss: 45.433275552433\n",
      "----\n",
      " , his fed tod to desciternish he ststatare thile heching to there wayoch whing souturne\n",
      " hell, Gregor's peverturstrature.\n",
      "\n",
      "Progod would the out neigh in more ag-te, you contwlect fiatering be and on w \n",
      "----\n",
      "iter 62000, loss: 43.64451099717333\n",
      "----\n",
      " rd a lowensswen lee he condiously went, shonding to be the\n",
      " he eroncesmorf in nis enop, but'th hit ard in he had thet bat ever toter beal. mo, taks in coucrentain duriifed littebontly ageed furst pove \n",
      "----\n",
      "iter 63000, loss: 41.92915840413471\n",
      "----\n",
      " chad for so, she of chundoong in homens that its pariing to to complesall Gregother had then day to faled as to was he sargs fiit would so cofersable to a disise witl inmishe reyill to rag beeth that  \n",
      "----\n",
      "iter 64000, loss: 41.61326891116997\n",
      "----\n",
      " ying he woul toome ofor hit. And berowns nouge getpaing comemer us the ofod exmeied undeandree have mackout that bibe caldad noting the these of she viom the lite to muthout he beaally appos been stre \n",
      "----\n",
      "iter 65000, loss: 41.4894228888387\n",
      "----\n",
      " her soon with react whith Gregor, uh have the gowly was uprietion osese and hingever. So ally room if and farclic frat hly and sanding-ding, in to hard, and ally roof dowrof had \"When the look dun ald \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 66000, loss: 45.182569675542595\n",
      "----\n",
      " unded perveryed you takes and pleif\n",
      "eefulais on the paitay turing to caPmonting\n",
      "lock prively of bealccleave all hive Poveroonss to flit stalcanbergonfound cid prepers sects atather'che\n",
      "Projet wive nov \n",
      "----\n",
      "iter 67000, loss: 44.41079211535305\n",
      "----\n",
      " ho deoscing dond in for cofcersting the downudgntionelar to your go, open roome seear withoc forCalless intlee for with whice jusw and clomer of the he oncher whan your Seed of and outnghst all, jug;  \n",
      "----\n",
      "iter 68000, loss: 42.33987882479452\n",
      "----\n",
      " ndach did had ash sundurly perister the reest not simith, sa%. Gregored all hamp hce go used, he he could thad bentiod. Ding of stots she she romearded to sleading a sase ad as fabit, said.  Greger hi \n",
      "----\n",
      "iter 69000, loss: 41.1719878773086\n",
      "----\n",
      " istery by when foomt hat dewwan the mive intlist it houn had sed over fome it paunked avo buct, Grejust but all, caaitely on onod the foll would in, fortainsion weren ass been dare the lainfy's they l \n",
      "----\n",
      "iter 70000, loss: 40.997731232595974\n",
      "----\n",
      " emed nefom wome alleg nettstyact now opling. Thearmss hakly down in lughtle, proomed to moting, bek hoten if he sloobl, it they he would mother door's not strmes courapt, youghyt with sidersostly come \n",
      "----\n",
      "iter 71000, loss: 42.71031603516636\n",
      "----\n",
      " .\n",
      "\n",
      "As the frars om sother a cliat cory wa cerpailer comect bow sucing, walloo all, mect. Qut perefarissitresting from at the bist the to to quioch more from they withant the may , thrie comss becars e \n",
      "----\n",
      "iter 72000, loss: 44.76117936516395\n",
      "----\n",
      " edent fed the work of the door badce shime one that heved prorg-tm Lide pereeckenbintle looked if votheadice tre ount one.\n",
      "ETh to stuinttore tood you all isk camain had a: mabsely there by it paate to \n",
      "----\n",
      "iter 73000, loss: 43.03568665977281\n",
      "----\n",
      " ly ut ondiis ablated hat she sirsessure haly nickeal from. He lemsly wand and an oned the booved of the fale tleaks, sonause ferto the a man tiNh as wous, ancoull himanckeast at to parpurableg dnead h \n",
      "----\n",
      "iter 74000, loss: 41.40775905238449\n",
      "----\n",
      " ed the kmebe way to rethew houghed shey had unde fout sted stet strentherswer. Buct to witled and sowwad and that and they the wained nom in oh poom to she in the lagr theation, a must had what asnept \n",
      "----\n",
      "iter 75000, loss: 41.16015775808729\n",
      "----\n",
      " tht semast in his excher rawe becher's simith to and whe byed. Founrisily ine any he was s rooth the crecanbe, as morn conccoichsed at is he him Gregorm coutears and the piget up in ow untremped om wo \n",
      "----\n",
      "iter 76000, loss: 40.95897923551394\n",
      "----\n",
      "  bat saldottreen the feltuinttimss bught not it leching\", here she erviokel tromemore there lent nos soncoud frock las work and hell?\"Y\"; suldet decraide was he hart, she, she parice geable woy'ln't l \n",
      "----\n",
      "iter 77000, loss: 44.41446564476098\n",
      "----\n",
      " to buccortionly As alroking, to perquinemazing and erom\n",
      "\" decront theneis becartated\n",
      "doying to this erost ponkly Oh if or was bednation\n",
      "(unshem. distaying, genolrout fueass intion nopriilles,\n",
      "1.n. Som \n",
      "----\n",
      "iter 78000, loss: 43.88612599361159\n",
      "----\n",
      "  wark incort to beens his mase nougher to quicet hile everysousted his rance, shis sonten felf, thenghat this cablear all the moblled at resion and of uf lact himse lota it whit eftr it entcorgay \"the \n",
      "----\n",
      "iter 79000, loss: 41.8519487252316\n",
      "----\n",
      " 's te looking the been do he pase had or aovisn that he postrine need it whice arrins.  aatel. Busted, botely heariaslirich one, he, pars at neire out even to ralt in.\n",
      "\n",
      "Liftle theing at was sake to th \n",
      "----\n",
      "iter 80000, loss: 40.73712904051581\n",
      "----\n",
      " timughed frock leat diving't erable. He kay ansiof hirgate necharon coreff\". By. Then one her and a feerew beccout; she both laye; in the weup for, ofeladed of toarthave siblien his sa out on thied th \n",
      "----\n",
      "iter 81000, loss: 40.56316220464916\n",
      "----\n",
      " room in in firt worsning, in fanding felk eremstortsturregor's fothation worbis mobleing thoopennar likan with then requhit voor with and and evencert. No tame to mould it sive ut was to to thes to th \n",
      "----\n",
      "iter 82000, loss: 42.12978731800918\n",
      "----\n",
      " owhil whook, must. Ands, arat susct comproute him the to and leck Gregor. SI aby, at babreat seneen the dourd who cray, the lopt by be onsinimsee to aimately forms - if eaduely out, \"Yourved., the the \n",
      "----\n",
      "iter 83000, loss: 44.18353909738903\n",
      "----\n",
      " hicked 6hay op of the comed he save Gregor evenxisacin hole them but ands the cory beniuld coverar (way so rooll pabce tell to somedition his figen his fatich and he robatelf  hic to lowin chang, thev \n",
      "----\n",
      "iter 84000, loss: 42.60533161070041\n",
      "----\n",
      " se sely the coore weistelees,\" with it.  as had inesling these him evered couplly his motice the coull agon'll watinen miok nounfyred he had auttearclmathing at he enow hand afyarien, the room wother  \n",
      "----\n",
      "iter 85000, loss: 40.97644358045744\n",
      "----\n",
      "  could had, beca:ly mister to kisl to comer wa loose; tether was lonin't was sty the hack Gutrote stoplent furns, blenies his necwings as dawwardert sholding have, the dooldre uven that he would sarks \n",
      "----\n",
      "iter 86000, loss: 40.798215662825406\n",
      "----\n",
      " verrowt; anke persiled more hat(ed ad him; thief clent of the ham acapplabis slone his even opt to fatintire her roulding and pars lask the mollefreaving, him himn stenes saition, the untion, from a t \n",
      "----\n",
      "iter 87000, loss: 40.5376371525559\n",
      "----\n",
      " g it lack, hast bock the'nd it it) had whice nid the dnough thene that apfemed droy, resen to mind let outlly. She she to have ham hoind horIR.\n",
      "\n",
      "THem to got and wimained as fore, hijustinis tood had v \n",
      "----\n",
      "iter 88000, loss: 43.81967479130592\n",
      "----\n",
      " his gate this worgs droject. Arm-bors, as fort gor the mivied faded of (ricord\n",
      "yoId aIned a one of the Projelppary thate forsed. He onstowing She lake of or Project a dorgin te ited\", ango times oprou \n",
      "----\n",
      "iter 89000, loss: 43.433605264523415\n",
      "----\n",
      " reisanes they made and go pereming in clention connyligeny allet, his flawinging cle tratichil cany, \"h'stally. I bunberstion to slid treep wriast and ronis the could with to her sist, act they carve. \n",
      "----\n",
      "iter 90000, loss: 41.432213072285634\n",
      "----\n",
      " ture out pay ondery went dounder in all the say, to erninftile at to been to have their as aved to then did foundings did inthis herw unition this buter to s meble fout the werest way tood paindryt an \n",
      "----\n",
      "iter 91000, loss: 40.4460811743849\n",
      "----\n",
      " But, inin just only meding thell, and rethen expen hip for, meh stagell the dide one flemsuse was you was lock.\n",
      "\n",
      "Grevers ass, buster, seren to as that to but turried and that shat, Gregor over for to  \n",
      "----\n",
      "iter 92000, loss: 40.199047038067405\n",
      "----\n",
      " ruthing they them semsing it calf farin the roong, suned they. And on give whin. \"got it at and wemsious tooking unxinatits and not the reat comenthe and and for bewore sive ham he laven. Choighar the \n",
      "----\n",
      "iter 93000, loss: 41.67193831218507\n",
      "----\n",
      " ittlenice.\n",
      "\n",
      "\"Yin having in the perporean for, it loDwers, in sop the timise to as of doally notiching, contrey comermef to dicl withel notely fecte.\n",
      "\n",
      "The tak took\n",
      "3 open on you in on his may. Gregor'd \n",
      "----\n",
      "iter 94000, loss: 43.769024676003795\n",
      "----\n",
      " his aren the truble bid has do klAticent his futhep hed inre, to he hand. 8uck of and the fle and un, off but bte to gink of there the copens\n",
      "padion to simations suielourId promn for wo Projectmonpyet \n",
      "----\n",
      "iter 95000, loss: 42.24038724412174\n",
      "----\n",
      " ting the - drece sab he celees wive harmed to ghis room in that intent.  fother in to other Swabled fos conlyse day, reepes. I caccontol, sary the fold in wergay at was see out it herreall as hord a m \n",
      "----\n",
      "iter 96000, loss: 40.65485032137456\n",
      "----\n",
      "  stilsters in ary her the protieno tearfught the fantido gry with thas prosictontt she like the beemened, the concersitidg ele, onolece shough they san and he sad she would that him evenick, eade thin \n",
      "----\n",
      "iter 97000, loss: 40.504628017768034\n",
      "----\n",
      " ive mutee when wher to that sooken to not, at betablie thems he dous in his not eghinlt, to dgother even thany wice the with his farl holl Gregor on would unhit. by call to chengsite that, work him. S \n",
      "----\n",
      "iter 98000, loss: 40.25482293675443\n",
      "----\n",
      "  but not azraidady.\"\n",
      "- sisping strat them, the stro work they, apperan they pabrugdly betult at held to comfort under and quite eventre.) then alfont purigh, theme mowcoms, to the the deall andmonnth  \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 99000, loss: 43.34136748015422\n",
      "----\n",
      " ed\n",
      "way to mupelS \"IAMEDIThIINTSINCICEISEVE PRENTCO ENT/ECT OR Y0RAGuted a pliap \"hich us of worked, parite pata?\n",
      "\n",
      "\"Sil\n",
      "wothin. BAbeced tingenth abyov mupeter. So Fou contmoting be this with a pally or \n",
      "----\n",
      "iter 100000, loss: 43.01869338532358\n",
      "----\n",
      " des to the foot that welt being loned cacent infoyh Gother any letad was ancores forbid clonging had farm and bechen fide, sty, dungo any in alid d as Gregor for go dive begraads no; af it wealf 1./\n",
      "P \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    # check \"How to feed the loss function to see how this part works\n",
    "    if p+seq_length+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "        p = 0 # go from start of data                                                                                                                                                             \n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # sample from the model now and then                                                                                                                                                        \n",
    "    if n % 1000 == 0:\n",
    "        print ('iter {}, loss: {}'.format(n, smooth_loss)) # print progress\n",
    "        sample(hprev, inputs[0], 200)\n",
    "\n",
    "    # perform parameter update with Adagrad                                                                                                                                                     \n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "    p += seq_length # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
